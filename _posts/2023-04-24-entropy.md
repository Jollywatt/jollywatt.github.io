---
layout: post
title: "Information entropy: my favourite proof"
author: "Joseph Wilson"
categories: math
tags: []
---

The information entropy

$$
H(p) = -K \sum_{i} p_i \log_2 p_i
$$

is the **unique** real-valued function of finite probability distributions which:

1. is continuous with respect to $$p$$,
2. obeys the _Composition Law_,
3. has some fixed value $$K$$ for a fair coin toss $$p = (\frac12, \frac12)$$.

Entropy is often motivated in strange and lofty ways, but this way is the simplest and clearest I’ve found.

## Motivating the Composition Law

If we want to define the ‘uncertainty’ associated with a distribution, what kind of properties should we want?

You can picture a finite probability distribution as a decision tree with weighted branches:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-1.png" width="50%">
</figure>

The same distribution can also be expressed by dividing the decision tree up into a composition of sub-trees:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-2.png" width="50%">
</figure>

Since these two pictures represent the same scenario, we want our uncertainty measure to be the same for both.
But how do you measure the total uncertainty in the second picture? By taking a _weighted sum_ of the uncertainties of each sub-tree.



<!-- It makes sense that you would _add_ together the uncertainties of each sub-tree to obtain total uncertainty, but why a _weighted_ sum? -->
Why a weighted sum?
Consider what should happen when one of the branches’ probabilities goes to zero: the uncertainty contributed from the sub-tree beneath that branch should also go to zero.

For example, probability of $$\{5, 6\}$$ is $$.1$$ in the picture, so the total uncertainty contains only a small contribution from the sub-tree stemming from it.

The composition law in this example gives

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-eqn.png" style="height: 5ex;">
</figure>

where the probabilities on each branch are too small to draw, but are still there!

### Formally

You can codify this composition law more formally. Let

$$
\begin{aligned}
p: Ω &→ [0, 1] \\
i &↦ p_i
\end{aligned}
$$

be a finite distribution satisfying $$\sum_{i ∈ Ω} p(i) = 1$$. Let $$\sim$$ be some equivalence relation on $$Ω$$. Define the quotient distribution

$$
\begin{aligned} 
p/{\sim}: Ω/{\sim} &→ [0, 1] \\
[i] &↦ \textstyle\sum_{j \sim i} p_j
\end{aligned}
$$

where $$[i]$$ is the equivalence class of $$i$$, and define the restrictions:

$$
\begin{aligned}
p|_{[i]} : [i] &→ [0, 1] \\
j &↦ \frac{p_j}{\sum_{k \sim j} p_k}
\end{aligned}
$$

These fit into the example above, where $$Ω = \{1, ..., 6\}$$ like so:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-3.png" width="90%">
</figure>

Expressed in this language, the composition law is

$$
H(p) = H(p/{\sim}) + \sum_{e ∈ Ω/{\sim}} (p/{\sim})(e) \, H(p|_e)
.$$


## Proof of Uniqueness

We will prove that any ‘uncertainty’ function $$H(p) = H(p_1, ..., p_n)$$, satisfying

1. continuity
2. the composition law
3. $$H(\frac12, \frac12) = K$$ for a chosen $$K$$

must be equal to the Shannon entropy

$$
H(p) = -K \sum_{i} p_i \log_2 p_i
.$$

The proof is in three steps:

1. Show that $$H(p)$$ is uniquely defined for all distributions $$p$$ if it is uniquely defined for all _rational_ distributions $$q ∈ ℚ^n$$.
2. Show that $$H(q)$$ is uniquely defined for all rational distributions $$q$$ if the entropy of the uniform distribution

    $$U(n) ≔ H(\underbrace{\textstyle\frac1n, ..., \frac1n}_n)$$

    is uniquely defined for all $$n$$. 
3. Show that $$U(n)$$ is uniquely defined for all $$n$$ if we fix $$U(2) = K$$.

### Step 1.

This follows by the assumption of continuity. If $$p ∈ ℝ^n$$, then $$H(p) ≔ \lim_i H(q_i)$$ where the $$q_i ∈ ℚ^n$$ are a sequence of rational distributions converging to $$p$$.

### Step 2.

Let $$q ∈ ℚ^n$$ be a rational distribution, and let $$D$$ be the lowest common denominator of all the probabilities $$q_i$$, so that

$$
q = (d_1/D, d_2/D, ..., d_n/D)
$$

where $$d_i$$ are non-negative integers.
Now consider the set $$Ω = \{1, 2, ..., D\}$$, and let $$r(i) = 1/D$$ be the uniform distribution on $$Ω$$.
Define an equivalence relation $$\sim$$ which partitions $$Ω$$ into $$n$$ different sets $$\{e_1, e_2, ..., e_n\}$$, where the $$i$$th set contains $$d_i$$ elements.
The size of the $$i$$th equivalence group, as a fraction of the whole, is given by $$d_i/D = q_i$$, so the we have $$r/{\sim} = q$$ by construction.

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-4.png" width="100%">
</figure>

From the composition law, we have

$$
\begin{aligned}
H(r) &= H(r/{\sim}) + \sum_{i = 1}^n \frac{d_i}D \, H(r|_{e_i}) \\
U(D) &= H(q) + \sum_{i=1}^n \frac{d_i}D \, U(d_i)
\end{aligned}
$$

and so we have shown that $$H(q) ≔ U(D) - \sum_{i=1}^n \frac{d_i}D \, U(d_i)$$ is uniquely defined by $$U(n)$$.


### Step 3.

We will now show that $$U(n)$$ is uniquely defined by $$U(2)$$ by showing that the only possible functions are

$$U(n) = K\log_2 n$$

where $$K$$ is a free parameter.

Consider a uniform distribution $$r$$ on $$\{1, 2, ..., nm\}$$ partitioned by $$\sim$$ into $$n$$ groups of $$m$$, so that the $$i$$th equivalence class is $$[ni] = \{ni, ni + 1, ni + m - 1\}$$.
Writing down the composition law for this partition yields

$$
\begin{aligned}
H(r) &= H(r/{\sim}) + \sum_{i = 1}^n \frac1n H(r|_{[ni]}) \\
U(nm) &= U(n) + \sum_{i = 1}^n \frac1n U(m)
\end{aligned}
$$

and hence $$U(nm) = U(n) + U(m)$$, in turn implying $$U(1) = 0$$.
The only functions with these properties are multiples of the logarithm.
By choosing $$U(2) ≔ K$$, we fix $$U(n) = K\log_2 n$$.


This completes the proof!