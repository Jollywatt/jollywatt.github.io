---
layout: post
title: "A Compelling Uniqueness Proof for Entropy"
author: "Joseph Wilson"
categories: math
coverimage: entropy-banner.png
blurb: |
    A neat characterisation of information entropy with a â€˜composition lawâ€™, and a uniqueness proof.
tags: []
---

The formula for information entropy

$$
H(ğ’‘) = -K \sum_{i} p_i \log_2 p_i
$$

is the **unique** real-valued function of finite probability distributions which:

1. is continuous with respect to $$ğ’‘$$,
2. obeys the _Composition Law_,
3. has some fixed value $$K$$ for a fair coin toss $$ğ’‘ = (\frac12, \frac12)$$.

There are many strange and lofty justifications for why entropy is defined the way it is.
My favourite by far is this uniqueness property, which involves a natural and rather pretty composition law.

## Motivating the Composition Law

If we want to define an â€˜uncertaintyâ€™ associated with a distribution, what properties should we want our measure to have?

You can picture a finite probability distribution as a decision tree with weighted branches:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-1.png" width="45%">
</figure>

The same distribution can also be expressed by dividing the decision tree up into a composition of sub-trees:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-2.png" width="50%">
</figure>

Notice that each final outcome still has the same overall probability â€” all weâ€™ve done is added an extra step.

Since these two pictures represent the same scenario, **we want our uncertainty measure to be the same for both.**
But how do you measure the total uncertainty in the second picture? By taking a _weighted sum_ of the uncertainties of each sub-tree.



<!-- It makes sense that you would _add_ together the uncertainties of each sub-tree to obtain total uncertainty, but why a _weighted_ sum? -->
Why a weighted sum?
Consider what should happen when one of the branchesâ€™ probabilities goes to zero: the total uncertainty should not contain the uncertainty of the sub-tree beneath that branch, since it certainly never occurs.

For example, probability of $$\{5, 6\}$$ is $$.1$$ in the picture, so the total uncertainty contains only a small contribution from the green sub-tree.

Thus, our uncertainty measure $$H$$ should satisfy

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-eqn.png" style="height: 5ex;">
</figure>

where the probabilities on each branch are too small to draw, but are still there!

### Formally

You can codify this composition law more formally. Let

$$
\begin{aligned}
ğ’‘: Î© &â†’ [0, 1] \\
i &â†¦ p_i
\end{aligned}
$$

be a finite probability distribution satisfying $$\sum_{i âˆˆ Î©} p_i = 1$$. Let $$\sim$$ be some equivalence relation on $$Î©$$. Define the _quotient_ distribution

$$
\begin{aligned} 
ğ’‘/{\sim}: Î©/{\sim} &â†’ [0, 1] \\
[i] &â†¦ \textstyle\sum_{j \sim i} p_j
\end{aligned}
$$

where $$[i]$$ is the equivalence class of $$i$$, and define the _restricted_ distributions:

$$
\begin{aligned}
ğ’‘|_{[i]} : [i] &â†’ [0, 1] \\
j &â†¦ \frac{p_j}{\sum_{k \sim j} p_k}
\end{aligned}
$$

These fit into the example above where $$Î© = \{1, ..., 6\}$$ like so:

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-3.png" width="90%">
</figure>

Expressed in this language, the composition law is

$$
H(ğ’‘) = H(ğ’‘/{\sim}) + \sum_{e âˆˆ Î©/{\sim}} (ğ’‘/{\sim})(e) \, H(ğ’‘|_e)
$$

where $$(ğ’‘/{\sim})(e) = \sum_{i âˆˆ e} p_i$$.


## Proof of Uniqueness

We will prove that any â€˜uncertaintyâ€™ function $$H(ğ’‘) = H(p_1, ..., p_n)$$, satisfying

1. continuity
2. the composition law
3. $$H(\frac12, \frac12) = K$$ for a specified $$K$$

must be equal to the Shannon entropy

$$
H(ğ’‘) = -K \sum_{i} p_i \log_2 p_i
.$$

The proof is in three steps:

1. Show that $$H(ğ’‘)$$ is uniquely defined for all distributions $$ğ’‘$$ if it is uniquely defined for all _rational_ distributions $$ğ’’ âˆˆ â„š^n$$.
2. Show that $$H(ğ’’)$$ is uniquely defined for all rational distributions $$ğ’’$$ if the entropy of the uniform distribution

    $$U(n) â‰” H(\underbrace{\textstyle\frac1n, ..., \frac1n}_n)$$

    is uniquely defined for all $$n$$. 
3. Show that $$U(n)$$ is uniquely defined for all $$n$$ if we fix $$U(2) = K$$.

### Step 1.

This follows by the assumption of continuity. If $$ğ’‘ âˆˆ â„^n$$, then $$H(ğ’‘) â‰” \lim_i H(ğ’’_i)$$ where the $$ğ’’_i âˆˆ â„š^n$$ are a sequence of rational distributions converging to $$ğ’‘$$.

### Step 2.

Let $$ğ’’ âˆˆ â„š^n$$ be a rational distribution, and let $$D$$ be the lowest common denominator of all the probabilities $$ğ’’_i$$, so that

$$
ğ’’ = (d_1/D, d_2/D, ..., d_n/D)
$$

where $$d_i$$ are non-negative integers.
Now consider the set $$Î© = \{1, 2, ..., D\}$$, and let $$ğ’“(i) = 1/D$$ be the uniform distribution on $$Î©$$.
Define an equivalence relation $$\sim$$ which partitions $$Î©$$ into $$n$$ different sets $$\{e_1, e_2, ..., e_n\}$$, where the $$i$$th set contains $$d_i$$ elements.
The size of the $$i$$th equivalence group, as a fraction of the whole, is given by $$d_i/D = q_i$$, so the we have $$ğ’“/{\sim} = ğ’’$$ by construction.

<figure>
    <img src="{{ site.github.url }}/assets/img/entropy-fig-4.png" width="100%">
</figure>

From the composition law, we have

$$
\begin{aligned}
H(ğ’“) &= H(ğ’“/{\sim}) + \sum_{i = 1}^n \frac{d_i}D \, H(ğ’“|_{e_i}) \\
U(D) &= H(ğ’’) + \sum_{i=1}^n \frac{d_i}D \, U(d_i)
\end{aligned}
$$

and so we have shown that $$H(ğ’’) â‰” U(D) - \sum_{i=1}^n \frac{d_i}D \, U(d_i)$$ is uniquely defined by $$U(n)$$.


### Step 3.

We will now show that $$U(n)$$ is uniquely defined by $$U(2)$$ by showing that the only possible functions are

$$U(n) = K\log_2 n$$

where $$K$$ is a free parameter.

Consider a uniform distribution $$r$$ on $$\{1, 2, ..., nm\}$$ partitioned by $$\sim$$ into $$n$$ groups of $$m$$, so that the $$i$$th equivalence class is $$[ni] = \{ni, ni + 1, ni + m - 1\}$$.
Writing down the composition law for this partition yields

$$
\begin{aligned}
H(ğ’“) &= H(ğ’“/{\sim}) + \sum_{i = 1}^n \frac1n H(ğ’“|_{[ni]}) \\
U(nm) &= U(n) + \sum_{i = 1}^n \frac1n U(m)
\end{aligned}
$$

and hence $$U(nm) = U(n) + U(m)$$, in turn implying $$U(1) = 0$$.
The only functions with these properties are multiples of the logarithm.
By choosing $$U(2) â‰” K$$, we fix $$U(n) = K\log_2 n$$.


This completes the proof!